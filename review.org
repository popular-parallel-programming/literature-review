#+STARTUP: hidestars
#+STARTUP: indent

#+TITLE: Declarative Parallel Array Programming and Spreadsheet End-User Development
#+AUTHOR: Florian Biermann
#+EMAIL: fbie@itu.dk

#+BIBLIOGRAPHY: array-programming-ieee-accepted unsrt
#+BIBLIOGRAPHY: array-programming-acm-accepted unsrt

* Declarative Parallel Array Programming
** O'Donnel, 1988: Architectures for declarative programming languages
Constant-time, functional array implementation. Lookup and update are
of constant time \cite{47507}.
** Grelck, 2007: Coordinating Data-Parallel SAC
Declarative streams in SAC. Explicit coordination of parallel
processing in a DSL inside SAC \cite{4228136}.
** Shafarenko, 1994: RETRAN
Gets rid of higher-order functions (skeletons) and automatically
applies functions to all elements of arrays if required by the type of
the function. Uses "anti-currying" of operands to achieve this, dual
of lifting \cite{367042}.
** Collins et al., 2014: NOVA
Functional declarative parallel language for homogeneous computing
backends. Inspired by NESL and DPH, uses vectorization. Shared-memory
multi-core backend displays linear speedup
\cite{Collins:2014:NFL:2627373.2627375}.
** Keller at el., 2010: Regular, Shape-Polymorphic Arrays
Avoids intermediate arrays in high-level declarative
programming. Introduces delayed arrays, which hinder sharing and
require more explicit knowledge about array state. Parallelism as part
of array evaluation \cite{Keller:2010:RSP:1863543.1863582}.
** Lippmeier et al., 2012: Work-Efficient Higher Order Vectorization
Flattening nested data parallelism without increasing asymptotic
complexity. Array representations and invariants for automated
flattening \cite{Lippmeier:2012:WEH:2364527.2364564}.
** Lippmeier et al., 2012: Guiding Parallel Array Fusion
Exposing array implementation via "type indices" to describe what
state the array is in (e.g. delayed, manifested) or what its
performance cost model is. Useful for end-programmers to reason about
their array states \cite{Lippmeier:2012:GPA:2364506.2364511}.
** Fluet et al., 2008: A Scheduling Framework for General-Purpose Languages
Implementation of a scheduling framework for a strict functional
language. Uses work-stealing to unwrap nested parallelism and shows
good performance. Argues for a mix of scheduling policies and
techniques to adapt to different problems.
** Bernecky & Scholz, 2015: Abstract Expressionism for Parallel Performance
Claim that terse, functional programs result in better performance
than hand-written, imperative code. They give experimental evidence by
comparing algorithms written in SAC and C.
** Walinsky and Banerjee, 1990: A Functional Programming Language for Massively Parallel Computers
Give inference rules for compiling "routing functions", which change
the layout of data, such as transposition or indexing to avoid
intermediate representations. They call this "structure inference".
** Henriksen & Oancea, 2014: Bounds Checking: An Instance of Hybrid Analysis
Describes how to remove bounds checking predicates from the
computation at compile time and how to specialize functions based on
these. The techniques are implemented in Futhark.
** Grelck & Scholz, 2007: SAC: Off-the-shelf Support for Data-Parallelism on Multicores
Description of the SAC implementation, its optimizing compiler,
scheduler and memory management. Work is modeled as micro-threads that
are small units of work. Micro-threads are managed via work-stealing
queues. Such a scheduling technique is required to optimize
performance, as all parallelism stems from array-operations that
require synchronization when done.
** Keller et al., 2012: Vectorization Avoidance
Flattening nested parallelism sometimes introduces arrays for
intendedly scalar values. These intermediate arrays require extra
allocation at a high cost. Subsequential fusion seems not able to
handle this. Therefore, it can pay off to avoid vectorization from the
beginning, for which this paper describes a promising technique.
** Bergstrom et al., 2010: Lazy Tree Splitting
Instead of performing static transformations, nested data-parallelism
can be achieved via scheduling and work-stealing. Choosing the right
work-size is problematic. They describe a work-stealing based method
to dynamically balance load across workers, relying heavily on
zippers.
** Deitz et al., 2003: The Design and Implementation of a Parallel Array Operator for Arbitrary Data Remapping
A remap-operator with scatter-gather semantics, implemented in
ZPL. The operator has strong semantics and can modify data layout
arbitrarily in arrays, while minimizing communication amongst
processors. Its expressiveness and its performance nearly equal to
hand-written Fortran make it very powerful.
** Maydan et al., 1993: Array Data-Flow Analysis and its Use in Array Privatization
The paper describes an inter-procedural data-flow analysis to detect
data-dependences in nested loops in imperative languages. The analysis
uses a Least-Write-Tree to determine when the last write operation to
an array subscript occurs and can thereby decide how to efficiently
share an array amongst processors.
** Hwang et al., 1995: An Array Operation Synthesis Scheme to Optimize Fortran 90 Programs
Fusion of array functions, such as transpose or reshape (i.e. remap)
in Fortran 90. Their technique fuses sequential expressions to avoid
temporaries and improves the performance for sequential programs. The
paper includes a very detailed overview of the applied
transformations.
** Stucki et al., 2015: RBB Vector: A Practical General Purpose Immutable Sequence
The authors describe a vector based on Relaxed-Radix-Balanced trees,
which exhibits effectively constant or amortized constant complexity
for all operations, e.g. indexing, updates and concatenation. For
array-operations, the parallel speedup however is sub-linear.
** Henriksen & Oancea, 2013: A T2 Graph-Reduction Approach to Fusion
To fuse SOAC's (second-order array combinator) efficiently, the
$\mathbb{L}_0$ language uses a number of fusion-rules that require a
control-flow-graph analysis on the input program. Thereby,
performance-degrading fusions can be avoided, e.g. if they introduce
duplicate computations. The rules are formulated as a computational
algebra. Similar in structure and spirit to
\cite{Chakravarty:2001:FAF:507635.507661}.
** Chakravarty & Keller, 2001: Functional Array Fusion
Formulates functional array fusion as array deforestation and gives
equational rewrite rules for flattening and fusion. Observes, that
flattening simplifies fusion greatly. Cites many foundational papers.
** Ching, 1990: Automatic Parallelization of APL-style Programs
Observes that APL can exploit functional and data parallelism, but
argues that, in order to parallelize, compilation is
necessary. Furthermore, Ching observes that scheduling and
partitioning of work pose problems and that parallelism in
array-languages is a matter of the user writing idiomatic code.
** Knobe & Sarkar, 1998: Array SSA Form and its Use in Parallelization
To parallelize imperative loops, the authors use a data-flow on the
static-single-assignment form of arrays. Their technique succeeds in
parallelizing all loops that are not inherently sequential due to
data-dependencies. This approach is geared towards iterative, explicit
loops. (Low relevance for declarative parallel programming.)
** Sinkarovs & Scholz, 2013: Semantics-Preserving Data Layout Transformations for Improved Vectorization
Proof and technique to re-arrange the layout of n-dimensional, nested
arrays for vectorization based on layout types, which can be
inferred. Promises fully automatic layout changes for improved
higher-order parallel primitives. Transformations seem to be large for
small functions already. (Must re-read for details.)
** Sastry & Clinger, 1994: Parallel Destructive Updating in Strict Functional Languages
Live-variable analysis can be used to detect cases in a functional
program, where destructive updates are allowed. An array variable that
is not live after some program point can be updated destructively. In
a similar fashion, partitioning and combining arrays can be analyzed
to optimize the overall program performance (cost of copying vs. cost
of pointing to old data). This analysis also allows parallel array
updates.
* Spreadsheet End-User Development
