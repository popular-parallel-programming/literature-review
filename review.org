#+STARTUP: hidestars
#+STARTUP: indent

#+TITLE: Declarative Parallel Array Programming and Spreadsheet End-User Development
#+AUTHOR: Florian Biermann
#+EMAIL: fbie@itu.dk

#+BIBLIOGRAPHY: array-programming-ieee-accepted unsrt
#+BIBLIOGRAPHY: array-programming-acm-accepted unsrt

* Declarative Parallel Array Programming
** O'Donnel, 1988: Architectures for declarative programming languages
Constant-time, functional array implementation. Lookup and update are
of constant time \cite{47507}.
** Grelck, 2007: Coordinating Data-Parallel SAC
Declarative streams in SAC. Explicit coordination of parallel
processing in a DSL inside SAC \cite{4228136}.
** Shafarenko, 1994: RETRAN
Gets rid of higher-order functions (skeletons) and automatically
applies functions to all elements of arrays if required by the type of
the function. Uses "anti-currying" of operands to achieve this, dual
of lifting \cite{367042}.
** Collins et al., 2014: NOVA
Functional declarative parallel language for homogeneous computing
backends. Inspired by NESL and DPH, uses vectorization. Shared-memory
multi-core backend displays linear speedup
\cite{Collins:2014:NFL:2627373.2627375}.
** Keller at el., 2010: Regular, Shape-Polymorphic Arrays
Avoids intermediate arrays in high-level declarative
programming. Introduces delayed arrays, which hinder sharing and
require more explicit knowledge about array state. Parallelism as part
of array evaluation \cite{Keller:2010:RSP:1863543.1863582}.
** Lippmeier et al., 2012: Work-Efficient Higher Order Vectorization
Flattening nested data parallelism without increasing asymptotic
complexity. Array representations and invariants for automated
flattening \cite{Lippmeier:2012:WEH:2364527.2364564}.
** Lippmeyer et al., 2012: Guiding Parallel Array Fusion
Exposing array implementation via "type indices" to describe what
state the array is in (e.g. delayed, manifested) or what its
performance cost model is. Useful for end-programmers to reason about
their array states \cite{Lippmeier:2012:GPA:2364506.2364511}.
** Fluet et al., 2008: A Scheduling Framework for General-Purpose Languages
Implementation of a scheduling framework for a strict functional
language. Uses work-stealing to unwrap nested parallelism and shows
good performance. Argues for a mix of scheduling policies and
techniques to adapt to different problems.
** Bernecky & Scholz, 2015: Abstract Expressionism for Parallel Performance
Claim that terse, functional programs result in better performance
than hand-written, imperative code. They give experimental evidence by
comparing algorithms written in SAC and C.
** Walinsky and Banerjee, 1990: A Functional Programming Language for Massively Parallel Computers
Give inference rules for compiling "routing functions", which change
the layout of data, such as transposition or indexing to avoid
intermediate representations. They call this "structure inference".
** Henriksen & Onacea, 2014: Bounds Checking: An Instance of Hybrid Analysis
Describes how to remove bounds checking predicates from the
computation at compile time and how to specialize functions based on
these. The techniques are implemented in Futhark.
** Grelck & Scholz, 2007: SAC: Off-the-shelf Support for Data-Parallelism on Multicores
Description of the SAC implementation, its optimizing compiler,
scheduler and memory management. Work is modeled as micro-threads that
are small units of work. Micro-threads are managed via work-stealing
queues. Such a scheduling technique is required to optimize
performance, as all parallelism stems from array-operations that
require synchronization when done.
** Keller et al., 2012: Vectorization Avoidance
Flattening nested parallelism sometimes introduces arrays for
intendedly scalar values. These intermediate arrays require extra
allocation at a high cost. Subsequential fusion seems not able to
handle this. Therefore, it can pay off to avoid vectorization from the
beginning, for which this paper describes a promising technique.
** Bergstrom et al., 2010: Lazy Tree Splitting
Instead of performing static transformations, nested data-parallelism
can be achieved via scheduling and work-stealing. Choosing the right
work-size is problematic. They describe a work-stealing based method
to dynamically balance load across workers, relying heavily on
zippers.
* Spreadsheet End-User Development
