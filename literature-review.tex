\documentclass[a4paper]{article}

\usepackage[square,sort,comma,numbers]{natbib}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{multicol}
\usepackage{todonotes}
\usepackage{url}

\lstset{
  breaklines=true,
  breakatwhitespace=true,
  basicstyle=\ttfamily,
}

\newcommand{\sac}{S\textsc{a}C}

\title{Systematic Mapping Study of Declarative Parallel Programming
  and Spreadsheet End-User Development}
\author{Florian Biermann\\\small{\texttt{fbie@itu.dk}}}

\begin{document}

\maketitle

\section{Introduction}
\label{sec:intro}

Spreadsheets are widely used in many different disciplines within
industry and science by domain experts who often are
non-programmers\todo{cite Sestoft 2014}. Research on spreadsheet
technology has recently gained increased interest, which is probably
due to a renewed popularity of functional programming
principles. Spreadsheets do not possess mutable state, which makes
reasoning about their correctness rather easy. Moreover, this
particular trait makes it possible to investigate transparent
parallelization of spreadsheet calculations.

In this systematic mapping study, we want to map out the publications
on spreadsheet end-user programming and declarative array programming
languages to inform further research on parallel programming in
spreadsheets. Declarative array programming allows a compiler to
re-write expressions to perform implicit parallelization of
computations. The user does not need to be aware of the parallelism
inherent to a problem and as long as they choose to use the highest
abstraction to model their computation, a sophisticated compiler can
infer such parallelism automatically.

With this study, we want to find\dots.\todo{Continue.}

\newpage{}
\part{A Systematic Mapping Study}
\label{part:syst-mapp-study}

\section{Protocol}
\label{sec:protocol}

\subsection{Background}
\label{sec:background}

The goal of this systematic mapping study is to coalesce two
apparently disjoint research areas, namely declarative parallel array
programming techniques and end-user development in a spreadsheet model
of computation. We base our methodology on~\citet{keele2007guidelines}
and~\citet{petersen2008systematic}.

This study is meant to motivate and inform further research on
declarative parallel programming in spreadsheets for end-users. We
want to gain an overview over well known techniques for transforming
expressions that describe seemingly sequential operations into
efficient parallel code. Furthermore, we want to gain insight into how
this can be combined with spreadsheet end-user development techniques
and if there has been made any effort into this direction
already. \todo{Should we maybe also include a search on spreadsheets
  AND parallel? It seems that there are only few publications on ACM
  and IEEE and then we have at least tried to find a connection.}

We purposely do not focus on data-flow parallelism in
spreadsheets. This is a separate project orthogonal to declarative
parallel array programming in spreadsheets.

To gain overlapping literature lists, we run three different search queries:

\begin{description}
\item[Declarative, parallel array programming languages] What are
  promising techniques for automatic parallelization of declarative,
  functional languages?
\item[Spreadsheet end-user development] Which paradigms have been
  developed, what do users use spreadsheets for?
\item[Parallelism in spreadsheets] To what degree have parallel
  spreadsheet engines been investigated? What are challenges and
  possibilities? Where does our agenda fit in?
\end{description}

\subsection{Research Questions}
\label{sec:research-questions}

Our research questions are listed in the following:

\begin{enumerate}
\item What is the state of the art in declarative parallel array
  programming languages?
\item What are the most scrutinized topics in array programming?
\item Has there been research on declarative parallel array
  programming in a spreadsheet model of computation?
\item Are there any obvious ways to combine declarative parallel array
  programming and spreadsheet end-user development?
\end{enumerate}

\subsection{Study Selection Criteria}
\label{sec:study-select-crit}

We perform two disjoint literature studies, one for declarative
parallel array programming languages and one for spreadsheet end-user
development.

In the following, we give a list of criteria for inclusion or
exclusion of studies. Naturally, some of the studies can fulfill
criteria of both lists. We choose therefore to perform a majority vote
on the number of fulfilled criteria when making a decision of
inclusion or exclusion.

\subsubsection{Criteria for Declarative Parallel Languages}
\label{sec:crit-decl-parall}

\paragraph{Inclusion Criteria}

A publication should be included if it:

\begin{itemize}
\item Mentions implementations of prominent array languages.
\item Focuses on automatic parallelization of array expressions.
\item Mentions caching and false sharing.
\item Talks about compiler optimizations.
\item Focuses on implementation of declarative, functional parallel
  programming techniques.
\item Mentions homogeneous systems and shared memory.
\item Mentions loop fusion and nested loops.
\item Talks generally about program transformation.
\end{itemize}

\paragraph{Exclusion Criteria}

A publication should be excluded if it:

\begin{itemize}
\item Focuses on the application of parallel programming, e.g.\ within
  machine learning.
\item Develops techniques for distributed memory or mentions message
  passing.
\item Develops techniques for focuses on formal verification.
\item Targets GPU, GPGPU, FPGA and hardware accelerated techniques on
  heterogeneous systems.
\item Includes I/O.
\item Works towards automatic parallelization of imperative languages.
\item Is a review paper without any novel contribution.
\item Mentions Accelerate.\todo{Cite Accelerate.}
\item Mentions storage, disk etc.
\item Focuses on transactional memory.
\end{itemize}

\subsubsection{Criteria for Spreadsheet End-User Development and
  Parallel Spreadsheets}
\label{sec:crit-spre-end}

\paragraph{Inclusion Criteria}

A publication should be included if it:

\begin{itemize}
\item Mentions functional programming or functional language.
\item Describes the implementation of a spreadsheet engine.
\item Focuses on gaining or providing spreadsheet understanding.
\item Mentions types or type-inference.
\item Is a survey of ``real-world spreadsheets'' and full spreadsheet corpora.
\end{itemize}

\paragraph{Exclusion Criteria}

A publication should be excluded if it:

\begin{itemize}
\item Focuses on the application of spreadsheets and specific
  spreadsheet models, such as simulations or in teaching.
\item Describes systems inspired by spreadsheets.
\item Mentions CSCW and knowledge work or performs ethnographic
  studies.
\item Focuses on data structures in spreadsheets.
\item Mentions either mashups, mobile apps or web development.
\item Focuses on external tools and architectures for spreadsheet
  users.
\item Develops techniques for transforming spreadsheets.
\end{itemize}

\subsection{Quality Assurance}
\label{sec:quality-assurance}

To verify that the results of our searches are meaningful, we compile
a list of relevant and important publications, which must be included
in the results obtained by our automated search\todo{Include
  Nlelloch's NESL, Peyton-Jones' User-Focused Approach to Functions
  and Sestoft's Spreadsheet Implementation Technology.}.

\section{Process}
\label{sec:process}

\subsection{Literature Search}
\label{sec:literature-search}

We have use IEEExplore and ACM Digital Library as sources for our
literature search. The number of results from these sources varies
drastically, which is probably due to IEEExplore interpreting search
queries very strictly. We avoid Google Scholar and CiteSeerX, as these
meta engines return way over a thousand publications for each query,
which is infeasible for our scope.

The uniqueness of a publication in the following means that a
publication is listed only once. Sometimes, publications have multiple
entries in a digital library, for instance one for a conference's
proceedings and one for SIGPLAN Notes.

We have an initial list of 681 publications to consider, which we
construct as described in the following:

\paragraph{Declarative Parallel Array Programming Languages}

To generate a literature list for declarative parallel array
programming languages, we use the following query:

\begin{lstlisting}
(functional AND array AND programming AND parallel) AND (data-parallel OR ``data parallel'' OR multi-core OR multicore  OR ``multi core'')
\end{lstlisting}

\noindent This results in 250 publications of which 194 are unique from the ACM
Digital Library. IEEExplore, however, only returns eight publications,
all of which are unique. This adds 202 publications to consider.

\paragraph{Spreadsheet End-User Development}

The search query we use for finding publications on spreadsheet
end-user development is:

\begin{lstlisting}
spreadsheets AND (end-user-development OR "end user development" OR "end-user development")
\end{lstlisting}

\noindent This results in 435 publications of which 386 are unique from the ACM
Digital Library and in 104 publications, all of which are unique, from
IEEExplore. This adds 424 publications to consider.

\paragraph{Parallel Spreadsheets}

We use the following query to generate a list of publications focusing
on anything parallel in spreadsheets:

\begin{lstlisting}
spreadsheets AND parallel
\end{lstlisting}

\noindent ACM returns 21 publications of which 17 are unique and
IEEExplore returns 38 publications. This adds 55 publications to
consider.

\subsection{Literature Selection}
\label{sec:literature-selection}

Using the criteria defined in Section~\ref{sec:study-select-crit}, we
include studies based on their titles and their abstracts. If the
title is not informative enough, we accept the study and will later
screen it again based on the abstract.

To perform the selection of literature move conveniently, we have
developed an
Emacs\footnote{\url{https://www.gnu.org/software/emacs/}}-based tool,
called the Systematic Literature Review Mode (SLIRM). SLIRM
automatically downloads abstracts and full-text files on demand for
BibTeX-formatted entries that have been exported from a digital
library, such as the ACM Digital Library. SLIRM is open-source and
freely available\footnote{Download SLIRM from
  \url{https://github.com/fbie/slirm}}.

\subsection{Mapping}
\label{sec:mapping}

In order to group publications and to map out the research field, we
assigned each publication at least one keyword.

\subsubsection{Declarative Parallel Programming}

The keywords for publications on declarative parallel array
programming are:

\begin{multicols}{2}

\begin{itemize}
\item APL
\item DPH
\item DSL
\item Data structures
\item Data-flow
\item Declarative
\item Destructive updates
\item Flattening
\item Fortran
\item Functional
\item Fusion
\item Inference rules
\item Loops
\item NESL
\item Nested Parallelism
\item Remap
\item SAC
\item Scheduling
\item Shape inference
\end{itemize}

\end{multicols}

\newpage{}
\part{Results}
\label{part:results}

\section{Declarative Parallel Programming}
\label{sec:declarative-parallel-programming}

We have identified four key topics within parallel array programming:
nested data parallelism, array fusion, data structures and loop
parallelization. While these are clearly interconnected and cannot
just be seen by themselves, we will use these to structure the review
of the research body on parallel array programming.

There are a few key-languages that appear in the literature. The two
oldest are Fortran\todo{Source?}, which is an imperative
high-performance language, and APL~\cite{Iverson1962Programming},
which is a high-level declarative and functional
language. Furthermore, much work has been done on Single-assignment-C
(\sac)\todo{Source?}, which is a C-like functional language, and Data
Parallel Haskell (DPH) and REPA\todo{Source?}, which are library and
compiler extensions for Haskell. Another very important language is
NESL~\cite{Blelloch1993NESL}, which introduced the idea of statically
flattening nested arrays.

The opportunity for parallelism in these languages stems from
bulk-operations over arrays. While imperative languages like Fortran
use iterative loops, functional languages depend on higher-order
functions like \texttt{map} or \texttt{fold} to express data-parallel
computations. \citet{Ching:1990:APA:97808.97826} argued that it is
easier for programmers to express parallelism in such a declarative
high-level way than by explicitly scheduling work to different
processors and implemented this approach in the APL370 compiler. The
only requirement to make use of such implicitly parallel constructs is
that the programmer writes idiomatic code which the language runtime
can parallelize
efficiently~\cite{Bernecky:2015:AEP:2774959.2774962}. Not only that
parallelism is much easier to extract, but also sequential programs
written in such a high-level style will often perform
faster~\cite{Bernecky:2015:AEP:2774959.2774962}.

\subsection{Nested Data Parallelism}
\label{sec:nest-data-parall}

Nested data parallelism describes the nesting of parallel operations
over a nested array. A standard example from the literature is matrix
multiplication~\cite{Keller:2010:RSP:1863543.1863582}. A matrix is
represented by an array of arrays. Matrix multiplication can then be
implemented conveniently via higher-order functions, here illustrated
in F\#:

\begin{lstlisting}[language=ML]
type Matrix = double [] [];
\end{lstlisting}
\todo{Actually write the code.}

Such a declarative implementation of matrix multiplication lends
itself naturally to an implicitly parallel execution. However, there
are possibilities for parallelism on different levels in the
three-dimensional array structure. Early work on the parallel language
Actulus~\cite{Perrott:1979:LAV:357073.357075} recognizes the
difficulty of nested data parallelism and as a temporary solution
restricts the level of parallelism to one dimension which the
programmer has to choose ahead of time.

A naive way of implementing such nested parallelism would be to simply
start new parallel threads from within each parallel thread. There
would be quite some overhead to this solution. Research on nested data
parallelism has focused on eliminating this overhead in two different
ways.

\paragraph{Flattening}

\citet{Blelloch1993Implementation} introduced the idea of statically
flattening nested arrays to perform an optimal parallelization of
operations over arrays. The key to doing so is the right choice of
array representation and there exists a representation which enables
the compiler to flatten arrays in constant time. This flattening has
been implemented in NESL and \citet{Blelloch:1996:PTS:232627.232650}
showed, based on NESL's built-in cost semantics, that NESL actually
can be implemented without overhead.

The idea of flattening nested parallelism has since been taken up
again by \citet{Lippmeier:2012:WEH:2364527.2364564} and moved to
Haskell. In contrast to NESL, Haskell is a fully-fledged functional
programming language with types and higher-order functions that is
compiled instead of interpreted.

As it turns out, flattening nested data parallelism is much harder to
achieve in such a higher-order language. The presence of higher-order
functions makes lifting of functions more
complicated~\cite{Lippmeier:2012:WEH:2364527.2364564}. Higher-order
flattening introduces many intermediate arrays that take time and
space. Therefore, \citet{Keller:2012:VA:2364506.2364512} developed a
technique to avoid such flattening if no performance increase can be
expected.

The more recent high-level language NOVA builds on the principles
developed in NESL and DPH, using many of such state-of-the-art
techniques and targets both, shared-memory multiprocessors as well as
general purpose GPUs~\cite{Collins:2014:NFL:2627373.2627375}.

Flattening nested data parallelism is tightly coupled to array fusion,
which we will discuss in detail in Section~\ref{sec:fusion}.

\paragraph{Dynamic Scheduling}

Static flattening nested data parallelism with no overhead is targeted
at homogeneous computations on possibly irregular arrays. Possible
imbalance of work distribution is mostly a result of irregular nested
data structures, such as sparse vectors where adjacent zero-valued
entries are not stored in memory. This demands a strong type system,
such that all elements of an array are of the same type. Otherwise, a
two-dimensional array might mostly contain numbers but spuriously also
other data types or arrays. This means that not the entire parallelism
is visible to the compiler, so static flattening might not be feasible
to balance the workload.

Dynamic scheduling leaves the distribution of work to run-time of the
program. \sac, for instance, uses work-stealing
queues~\cite{Chase2005Dynamic, Grelck:2007:SOS:1248648.1248654} to
dynamically schedule work. Such scheduling schemes can produce some
overhead at run-time that would otherwise be alleviated by
compile-time transformations. To minimize overhead for different kinds
of computations, \citet{Fluet:2008:SFG:1411204.1411239} argue for a
mix of schedulers and to let the run-time chose which scheduler to use
for distributing work.

A notable scheduling heuristic is lazy tree
splitting~\cite{Bergstrom:2010:LTS:1863543.1863558}. In this scheme,
every thread gets assigned some part of the workload. Threads
communicate with other threads via a technique inspired by and as
efficient as work-stealing queues. Arrays are represented via binary
trees of arrays. For each portion of work that a thread performs, it
checks if any of the remaining threads are idle. If so, it splits the
remaining array in half, dispatches the second part to the idle
threads and continues to the next portion of work. This heuristic
shows low overhead in experimental
settings~\cite{Bergstrom:2010:LTS:1863543.1863558}.

\subsection{Fusion}
\label{sec:fusion}

Fusion refers to avoiding intermediate representations of arrays for
consecutive bulk operations. For instance, two succeeding applications
of \texttt{map} can be fused as follows:

\begin{center}
  \begin{tabular}{ccc}
\begin{lstlisting}[language=ML]
map g (map f xs)
\end{lstlisting}
    & $\Longrightarrow$ &
\begin{lstlisting}[language=ML]
map (f >> g) xs
\end{lstlisting}
  \end{tabular}
\end{center}

This optimization is very valuable in all kinds of programs,
sequential and parallel alike, for example in sequential
Fortran~90 programs\cite{Hwang:1995:AOS:209936.209949}.

Fusion is a static technique performed during compile time. The fusion
transformations can be expressed as straight-forward equational
rewrite rules, as done by
\citet{Chakravarty:2001:FAF:507635.507661}. They also observe that
flattening nested arrays makes fusion a much simpler task which
emphasizes the connection between flattening and fusion. Some
researchers have been focusing on making such optimizations visible to
the programmer via
types~\cite{Lippmeier:2012:GPA:2364506.2364511}. This enables
programmers to reason about the performance of their declarative code.

More aggressive fusion is possible if the compiler performs a more
complex analysis. \citet{Henriksen:2013:TGA:2502323.2502328} use a
data-flow based graph-reduction to analyze functional programs with
second-order functions on arrays for fusion possibilities. Their
analysis can detect code structures that inhibit fusion, subsequently
re-writes the program in such a way that fusion becomes possible and
avoids duplicate computations

A special variant of fusion is destructive update analysis. Since data
structures are immutable in purely functional languages, writing to a
single index of an array produces in a new array in which all elements
are copied from the original array with exception of the index that
was updated. If the array that is being updated is not referenced
again throughout the program, the update can be performed in-place, or
destructively, without the overhead of copying all
data. \citet{Sastry:1994:PDU:182409.182486} developed an analysis for
destructive array updates using live-variable data-flow analysis.

\subsection{Data Structures}
\label{sec:data-structures}

\cite{47507, Keller:2010:RSP:1863543.1863582,
  Stucki:2015:RVP:2784731.2784739, Lowney:1981:CAI:567532.567533,
  Hall:1994:UHT:182409.156781, Arvind:1989:IDS:69558.69562}\todo{Write about these publications.}

\subsection{Loop Parallelization}
\label{sec:loop-parallelization}

\cite{Tang:1990:CTD:77726.255155, Maydan:1993:AFA:158511.158515,
  Knobe:1998:ASF:268946.268956,
  Henriksen:2014:BCI:2627373.2627388}\todo{Write about these
  publications.}

\subsection{Other Topics}
\label{sec:other-topics}

\paragraph{DSLs, coordination and explicit scheduling}

\cite{4228136, Ureche:2012:SCS:2103746.2103762}

\paragraph{RETRAN}

\cite{367042}

\paragraph{Remap and data layout}

\cite{Walinsky:1990:FPL:91556.91610, Deitz:2003:DIP:781498.781526,
  Sinkarovs:2013:SDL:2502323.2502332}.

\section{Spreadsheet End-User Development}
\label{sec:spreadsheet-end-user-dev}

\bibliographystyle{unsrtnat}
\bibliography{array-programming-acm-accepted,array-programming-ieee-accepted,fbie}

\end{document}
