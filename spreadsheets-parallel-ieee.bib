@INPROCEEDINGS{5571340,
  author =	 {Andrews, K. and Lessacher, M.},
  booktitle =	 {Information Visualisation (IV), 2010 14th
                  International Conference},
  title =	 {Liquid Diagrams: Information Visualisation Gadgets},
  year =	 2010,
  pages =	 {104-109},
  abstract =	 {Information visualisation techniques have sometimes
                  been slow to diffuse into more widespread public
                  use. Recent advances in cloud computing have opened
                  up opportunities to bring information visualisation
                  to the masses in ways previously not
                  possible. Liquid diagrams are a suite of information
                  visualisation gadgets written in Flex, which
                  visualise live data contained in Google Docs
                  spreadsheets through the Google Visualization
                  API. Users can interactively configure the
                  visualisation and any changes in the online
                  spreadsheet data are reflected immediately in the
                  display. In contrast to other solutions, liquid
                  diagrams gadgets specifically support the printing
                  and export of both vector (SVG) and raster (PNG)
                  graphics versions of the visualisations, allowing
                  users to construct and export high-quality diagrams
                  for inclusion into other works. The suite of
                  visualisation gadgets currently available includes:
                  area charts, bar charts, heat maps (choropleths),
                  line charts, pie charts, treemaps, and parallel
                  coordinates plots. Star plots and voronoi treemaps
                  are coming soon.},
  keywords =	 {Internet;application program interfaces;data
                  visualisation;spreadsheet programs;Google docs
                  spreadsheets;Google visualization API;cloud
                  computing;information visualisation gadgets;liquid
                  diagrams;online spreadsheet data;Containers;Data
                  visualization;Flexible printed
                  circuits;Google;Heating;Image color analysis;XML},
  doi =		 {10.1109/IV.2010.100},
  ISSN =	 {1550-6037},
  month =	 {July},
}

@INPROCEEDINGS{4126358,
  author =	 {Turner, R.},
  booktitle =	 {Reliability and Maintainability Symposium,
                  2007. RAMS '07. Annual},
  title =	 {Optimized Allocation of Testing Budget for Missile
                  Defense Vehicle},
  year =	 2007,
  pages =	 {250-253},
  abstract =	 {ARES Corporation was asked to help analyze the
                  reliability of a subsystem composed only of demand
                  type components ("one-shot" pyrotechnic devices) for
                  a missile defense vehicle. The subsystem was
                  composed of twenty component types and some seventy
                  component instances arranged in various series
                  parallel blocks. Each component was considered to be
                  only as reliable as indicated by a relatively small
                  set of tests in a relevant environment. ARES
                  formulated a conjugate-pair Bayesian analysis
                  approach, in which each component was associated
                  with a uniform prior distribution on (0,1) -
                  representing no information about
                  reliability-combined with a binomial likelihood
                  function on n tests and f failures. The result is a
                  Beta posterior distribution on component
                  reliability, which can be calculated easily in a
                  spreadsheet. A Monte Carlo analysis on the system
                  logic then derives the probability distribution at
                  the system level. The client required that the
                  testing budget be allocated to maximize the payoff
                  in expected system reliability. The solution was an
                  optimization algorithm which identifies, after each
                  additional test has been added, that component which
                  will yield the largest reliability gain per dollar
                  at the system level given a single additional
                  successful test. The model thus considers both the
                  component reliability and the cost of an additional
                  test on that component when designing a test
                  plan. The current model evaluates the optimum
                  allocation of tests among the various components to
                  prove that the system has a specified reliability at
                  a given confidence level, assuming that no failures
                  occur in the additional tests. This methodology can
                  be extended to account for a non-zero number of
                  failures in additional tests, though at considerable
                  expense in computation time},
  keywords =	 {Bayes methods;Monte Carlo methods;budgeting;defence
                  industry;optimisation;reliability;ARES
                  Corporation;Beta posterior distribution;Monte Carlo
                  analysis;binomial likelihood function;component
                  reliability;conjugate-pair Bayesian analysis
                  approach;missile defense vehicle;optimized
                  allocation;probability distribution;pyrotechnic
                  devices;testing budget;Bayesian methods;Failure
                  analysis;Information analysis;Logic;Missiles;Monte
                  Carlo methods;Probability
                  distribution;Reliability;System testing;Vehicles},
  doi =		 {10.1109/RAMS.2007.328063},
  ISSN =	 {0149-144X},
  month =	 {Jan},
}

@INPROCEEDINGS{4271963,
  author =	 {Itoh, M. and Fujima, J. and Ohigashi, M. and Tanaka,
                  Y.},
  booktitle =	 {Information Visualization, 2007. IV '07. 11th
                  International Conference},
  title =	 {Spreadsheet-based Framework for Interactive 3D
                  Visualization of Web Resources},
  year =	 2007,
  pages =	 {65-73},
  abstract =	 {We propose a spreadsheet-based visualization
                  framework for end-users to generate and modify
                  multiple 3D visualizations of data-sets from various
                  Web resources. In this paper, first, we provide a 3D
                  component-based access mechanism to Web
                  resources. It allows users to access various Web
                  resources using only 3D visual components
                  interactively. Second, we provide an interactive 3D
                  visualization mechanism. It enables users to
                  construct multiple 3D visualizations of data-sets
                  from various Web resources just by combining 3D
                  visual components. Third, we provide 2D components
                  for communicating between a spreadsheet and 3D
                  components. Users can export necessary functions of
                  3D components and define synchronization between
                  these components and cells on a spreadsheet. A
                  spreadsheet environment allows us to define
                  relationships among cells, and copy these
                  relationships through a copy and paste
                  manipulation. By using these mechanisms, users can
                  create multiple visualizations in parallel in order
                  to compare different visualization results
                  simultaneously for different visualization
                  parameters just through a user's direct
                  manipulation.},
  keywords =	 {Internet;data visualisation;human computer
                  interaction;interactive systems;Web resource
                  access;interactive 3D visual component-based access
                  mechanism;interactive 3D
                  visualization;spreadsheet-based visualization
                  framework;Data mining;Data visualization;Graphical
                  user interfaces;Information filtering;Information
                  filters;Interactive
                  systems;Laboratories;Proteins;Virtual
                  environment;Web services},
  doi =		 {10.1109/IV.2007.106},
  ISSN =	 {1550-6037},
  month =	 {July},
}

@INPROCEEDINGS{82138,
  author =	 {Walter, D.C. and McMillan, M.M.},
  booktitle =	 {Applied Computing, 1990., Proceedings of the 1990
                  Symposium on},
  title =	 {A spreadsheet method for studying neural networks},
  year =	 1990,
  pages =	 {42-44},
  abstract =	 {A unified framework and method for studying small
                  neural networks (up to 75 neurons) using a computer
                  spreadsheet is described. Neural networks actually
                  resemble spreadsheets in several ways. A neural
                  network consists of many simple computational units,
                  highly interconnected and operating in
                  parallel. Each unit has a numerical value (an
                  output), which it communicates to other units along
                  connections of varying strength. Similarly, a
                  spreadsheet contains several thousand cells,
                  arranged in rows and columns, appearing to perform
                  in parallel. The numerical values of certain cells
                  (i.e. their outputs) become parameters for
                  calculating the values of others linked to them via
                  suitable formulas. Just as the units of most
                  networks are identical to each other, the formulas
                  of spreadsheet cells are often highly repetitive,
                  except for the relative location of cells which they
                  reference. The authors do not advocate that all
                  artificial neural networks be implemented on a
                  spreadsheet. However, the spreadsheet is a valuable
                  research tool and learning aid},
  keywords =	 {neural nets;parallel architectures;spreadsheet
                  programs;virtual machines;artificial neural
                  networks;cells;computer spreadsheet;highly
                  interconnected;learning aid;numerical
                  values;parallel;research tool;simple computational
                  units;small neural networks;spreadsheet
                  method;unified framework;Artificial neural
                  networks;Computational modeling;Computer
                  networks;Computer
                  simulation;Hospitals;Keyboards;Network
                  topology;Neural networks;Neurons;Pediatrics},
  doi =		 {10.1109/SOAC.1990.82138},
  month =	 {Apr},
}

@INPROCEEDINGS{590455,
  author =	 {Sute Lei and Kang Zhang},
  booktitle =	 {Parallel and Distributed Systems,
                  1994. International Conference on},
  title =	 {Performance tuning of message passing programs
                  through visual analysis},
  year =	 1994,
  pages =	 {730-735},
  abstract =	 {Designing a parallel program to fully utilise the
                  processing power of a multiprocessor machine
                  requires a series of performance analysis and
                  tuning. The paper describes a performance tuning
                  tool for message passing parallel programs. The tool
                  combines the advantages of relational databases and
                  spreadsheets to organise the performance data and
                  analyse the program performance through
                  visualisation. Various graphical displays which
                  assist the user to fine tune the performance of
                  message passing programs are discussed},
  keywords =	 {message passing;parallel programming;program
                  diagnostics;software performance evaluation;visual
                  programming;graphical displays;message passing
                  programs;multiprocessor machine;parallel
                  program;parallel programs;performance
                  analysis;performance data;performance
                  tuning;processing power;relational
                  databases;spreadsheets;tuning tool;visual
                  analysis;Data analysis;Data
                  visualization;Debugging;Displays;Instruments;Message
                  passing;Monitoring;Multiprocessing
                  systems;Performance analysis;Relational databases},
  doi =		 {10.1109/ICPADS.1994.590455},
  month =	 {Dec},
}

@INPROCEEDINGS{4117880,
  author =	 {de Mesquita, M.A. and Hernandez, A.E.},
  booktitle =	 {Simulation Conference, 2006. WSC 06. Proceedings of
                  the Winter},
  title =	 {Discrete-Event Simulation of Queues with
                  Spreadsheets: A Teaching Case},
  year =	 2006,
  pages =	 {2277-2283},
  abstract =	 {This paper describes the use of spreadsheets
                  combined with simple VBA code as a tool for teaching
                  and discrete-event simulation. Four different cases
                  are considered: single server, parallel servers,
                  tandem queuing, and closed queuing system. The data
                  obtained in the simulation run are conveniently
                  stored in spreadsheets for subsequent statistical
                  analysis. This approach was successfully deployed in
                  a second one-semester course on management science
                  for industrial engineers undergraduate students},
  keywords =	 {computer aided instruction;computer science
                  education;discrete event simulation;management
                  science;queueing theory;spreadsheet
                  programs;statistical analysis;teaching;closed
                  queuing system;discrete-event simulation;management
                  science;parallel servers;queuing theory;single
                  server;spreadsheets;statistical analysis;Analytical
                  models;Computer aided software engineering;Discrete
                  event simulation;Education;Engineering
                  management;Logic programming;Mathematical
                  model;Production engineering;Queueing
                  analysis;Statistical analysis},
  doi =		 {10.1109/WSC.2006.323053},
  month =	 {Dec},
}

@INPROCEEDINGS{6568505,
  author =	 {Ammeri, A. and Dammak, M.C. and Chabchoub, H. and
                  Hachicha, W. and Masmoudi, F.},
  booktitle =	 {Advanced Logistics and Transport (ICALT), 2013
                  International Conference on},
  title =	 {A simulation optimization approach-based genetic
                  algorithm for lot sizing problem in a MTO sector},
  year =	 2013,
  pages =	 {476-481},
  abstract =	 {In this paper, a combined simulation and Genetic
                  Algorithm (GA) optimization model is developed to
                  solve the Lot Sizing Problem (LSP) in a Make to
                  Order (MTO) supply chain. The simulation model is
                  performed using ARENA software. GA model is
                  implemented using Visual Basic for Application (VBA)
                  language, because it ensures exchanges between ARENA
                  software and Ms Excel. The GA and simulation models
                  operate in parallel over time with interactions. The
                  case study's objective is to determine a fixed
                  optimal lot size for each manufactured product type
                  that will ensure order mean flow time target for
                  each finished product. The comparative results with
                  OptQuest software, which is used a global search
                  method, to illustrate the efficiency and
                  effectiveness of the proposed approach.},
  keywords =	 {Visual BASIC;genetic algorithms;lot sizing;order
                  processing;product customisation;production
                  engineering computing;search problems;spreadsheet
                  programs;supply chains;ARENA software;GA;LSP;MTO
                  sector;Ms Excel;OptQuest software;VBA;Visual Basic
                  for application language;global search method;lot
                  sizing problem;make to order supply
                  chain;manufactured product type;order mean flow time
                  target;simulation optimization approach-based
                  genetic algorithm;Computational modeling;Genetic
                  algorithms;Object oriented
                  modeling;Optimization;Sociology;Statistics;Supply
                  chains;Case study;Genetic Algorithm;Lot sizing
                  problem;Make to order supply chain;Simulation
                  Optimization;VBA},
  doi =		 {10.1109/ICAdLT.2013.6568505},
  month =	 {May},
}

@INPROCEEDINGS{631884,
  author =	 {Herbordt, M.C. and Anand, A. and Kidwai, O. and Sam,
                  R. and Weems, C.C.},
  booktitle =	 {Computer Architecture for Machine Perception,
                  1997. CAMP 97. Proceedings. 1997 Fourth IEEE
                  International Workshop on},
  title =	 {Processor/memory/array size tradeoffs in the design
                  of SIMD arrays for a spatially mapped workload},
  year =	 1997,
  pages =	 {12-21},
  abstract =	 {Though massively parallel SIMD arrays continue to be
                  promising for many computer vision applications,
                  they have undergone few systematic empirical
                  studies. The problems include the size of the
                  architecture space, the lack of portability of the
                  test programs, and the inherent complexity of
                  simulating up to hundreds of thousands of processing
                  elements. The latter two issues have been addressed
                  previously, here we describe how spreadsheets and
                  tk/tcl are used to endow our simulator with the
                  flexibility to model a large variety of designs. The
                  utility of this approach is shown in the second half
                  of the paper where results are presented as to the
                  performance of a large number of array size,
                  datapath, register file, and application code
                  combinations. The conclusions derived include the
                  utility of multiplier and floating point support,
                  the cost of virtual PE emulation, likely
                  datapath/memory combinations, and overall designs
                  with the most promising performance/chip area
                  ratios},
  keywords =	 {computer vision;floating point arithmetic;parallel
                  architectures;spreadsheet programs;application
                  code;architecture space;computer vision
                  applications;datapath/memory combinations;floating
                  point support;massively parallel SIMD
                  arrays;performance/chip area
                  ratios;processor/memory/array size
                  tradeoffs;register file;spatially mapped
                  workload;spreadsheets;tk/tcl;Application
                  software;Computational modeling;Computer
                  architecture;Computer
                  vision;Contracts;Costs;Emulation;Laboratories;Registers;Testing},
  doi =		 {10.1109/CAMP.1997.631884},
  month =	 {Oct},
}

@INPROCEEDINGS{374440,
  author =	 {Masa, P. and Hoen, K. and Wallinga, H.},
  booktitle =	 {Neural Networks, 1994. IEEE World Congress on
                  Computational Intelligence., 1994 IEEE International
                  Conference on},
  title =	 {70 input, 20 nanosecond pattern classifier},
  year =	 1994,
  volume =	 3,
  pages =	 {1854-1859 vol.3},
  abstract =	 {A CMOS neural network integrated circuit is
                  discussed, which was designed for very high speed
                  applications. This full-custom, mixed analog-digital
                  chip implements a fully connected feedforward neural
                  network with 70 inputs, 6 hidden layer neurons and
                  one output neuron. The neurons perform inner product
                  operation and have a sigmoid-like activation
                  function. The 70 network inputs and the neural
                  signal processing are analog, the synaptic weights
                  are digitally programmable with 5 bit (4 bits+sign)
                  precision. The synaptic weights are stored on
                  on-chip static RAM cells. The combination of analog
                  and digital techniques results in unique computing
                  power with ease of use. Programming can easily be
                  performed with the help of a spreadsheet or other
                  suitable interface program from a PC. The resolution
                  of the input signals is mainly determined by the
                  signal to noise ratio which lies typically between
                  8-12 bits. Therefore the equivalent input bandwidth
                  can be as high as 28-42 Gbits/second. The system is
                  designed for very high speed vector classification
                  and the feasibility of a single chip neural network
                  photon trigger for nuclear research is
                  shown. Because of the fully parallel architecture
                  and the fast analog signal processing the network
                  achieves unique computing performance and classifies
                  up to 70 dimensional vectors within 20 nanoseconds,
                  performing 20 billion (2×1010) multiply-and-add
                  operations per second. The circuit occupies 10×9 mm2
                  silicon area with 1.5 μm CMOS process and dissipates
                  only 1 W at 5 V supply},
  keywords =	 {CMOS integrated circuits;SRAM chips;detector
                  circuits;feedforward neural nets;high energy physics
                  instrumentation computing;mixed analogue-digital
                  integrated circuits;neural chips;neural net
                  architecture;nuclear electronics;pattern
                  classification;trigger circuits;very high speed
                  integrated circuits;1 W;1.5 μm CMOS process;1.5
                  mum;20 ns;28 to 42 Gbit/s;5 V;70 input 20 nanosecond
                  pattern classifier;CMOS neural network integrated
                  circuit;digitally programmable synaptic weights;fast
                  analog signal processing;full-custom mixed
                  analog-digital chip;fully connected feedforward
                  neural network;fully parallel architecture;neural
                  signal processing;nuclear research;on-chip static
                  RAM cells;sigmoid-like activation function;single
                  chip neural network photon trigger;very high speed
                  vector classification;Analog
                  computers;Analog-digital conversion;Application
                  specific integrated circuits;CMOS integrated
                  circuits;Digital signal processing chips;Feedforward
                  neural networks;High speed integrated
                  circuits;Neural networks;Neurons;Very high speed
                  integrated circuits},
  doi =		 {10.1109/ICNN.1994.374440},
  month =	 {Jun},
}

@INPROCEEDINGS{477403,
  author =	 {Lew, A. and Halverson, R., Jr.},
  booktitle =	 {FPGAs for Custom Computing Machines,
                  1995. Proceedings. IEEE Symposium on},
  title =	 {A FCCM for dataflow (spreadsheet) programs},
  year =	 1995,
  pages =	 {2-10},
  abstract =	 {We show how the University of Hawaii's field
                  programmable gate array (FPGA) based custom machine
                  (FCCM) can be used for dataflow programs expressed
                  as spreadsheets. We also describe some nontrivial
                  applications of this dataflow spreadsheet
                  FCCM. These applications include computing the
                  discrete cosine transform and solving dynamic
                  programming problems},
  keywords =	 {computer architecture;data flow computing;dynamic
                  programming;field programmable gate arrays;parallel
                  programming;spreadsheet programs;FCCM;FPGA based
                  custom machine;dataflow spreadsheet
                  programs;discrete cosine transform;dynamic
                  programming problems;field programmable gate
                  array;nontrivial applications;Application
                  software;Computer applications;Computer
                  architecture;Computer
                  languages;Coprocessors;Discrete cosine
                  transforms;Dynamic programming;Field programmable
                  gate arrays;Programming profession;Random access
                  memory},
  doi =		 {10.1109/FPGA.1995.477403},
  month =	 {Apr},
}

@ARTICLE{4302705,
  author =	 {Dadda, L.},
  journal =	 {Computers, IEEE Transactions on},
  title =	 {Multioperand Parallel Decimal Adder: A Mixed Binary
                  and BCD Approach},
  year =	 2007,
  volume =	 56,
  number =	 10,
  pages =	 {1320-1328},
  abstract =	 {Decimal arithmetic has been revived in recent years
                  due to the large amount of data in commercial
                  applications. We consider the problem of
                  multioperand parallel decimal addition with an
                  approach that uses binary arithmetic, suggested by
                  the adoption of binary-coded decimal (BCD)
                  numbers. This involves corrections in order to
                  obtain the BCD result or a binary-to-decimal (BD)
                  conversion. We adopt the latter approach, which is
                  particularly efficient for a large number of
                  addends. Conversion requires a relatively small area
                  and can afford fast operation. The BD conversion
                  moreover allows an easy alignment of the sums of
                  adjacent columns. We treat the design of BCD digit
                  adders using fast carry-free adders and the
                  conversion problem through a known parallel scheme
                  using elementary conversion cells. Spreadsheets have
                  been developed for adding several BCD digits and for
                  simulating the BD conversion as a design tool.},
  keywords =	 {adders;binary codes;digital arithmetic;binary
                  arithmetic;binary-coded decimal;binary-to-decimal
                  conversion;decimal arithmetic;elementary conversion
                  cells;multioperand parallel decimal
                  adder;Adders;Computational modeling;Digital
                  arithmetic;Embedded
                  system;Hardware;Internet;Law;Legal factors;Market
                  research;Parallel processing;Computer
                  arithmetic;decimal arithmetic;hardware
                  design;multioperand adders},
  doi =		 {10.1109/TC.2007.1067},
  ISSN =	 {0018-9340},
  month =	 {Oct},
}

@INPROCEEDINGS{265046,
  author =	 {Rogne, T. and Mo, O. and Schlurscheid, M.},
  booktitle =	 {Power Electronics and Applications, 1993., Fifth
                  European Conference on},
  title =	 {Paralleling of semiconductors including temperature
                  feedback, using spreadsheet or simulation tool, to
                  calculate current and temperature differences},
  year =	 1993,
  pages =	 {149-154 vol.2},
  abstract =	 {Paralleling of semiconductors like IGBTs and diodes
                  is complicated due to the usually negative
                  temperature coefficient of forward voltage drop, and
                  positive temperature coefficient of switching
                  losses. The authors state which component knowledge
                  is necessary as data input and then present two
                  efficient tools for calculating the current and
                  temperature differences of the paralleled
                  semiconductor chips. When using either of the two
                  tools, a spreadsheet or circuit simulation program,
                  the feedback from the temperature is included
                  through iterations. It is shown that for instance a
                  20% imbalance in voltage drop and switching loss may
                  demand a derating down to 32% in a typical chopper
                  application at 20 kHz. With a sinusoidal current, as
                  in a motor inverter, with 20% imbalance, a derating
                  down to 69% is sufficient. Large switching versus
                  conduction losses are very bad when paralleling
                  typical bipolar semiconductors. When parallelling,
                  it is optimal with a faster semiconductor, or a
                  lower frequency, compared to when using a single
                  switch. The authors also give measured data for
                  various IGBTs, and illustrations of measured current
                  sharing between paralleled IGBTs},
  keywords =	 {digital simulation;electronic engineering
                  computing;insulated gate bipolar
                  transistors;losses;power electronics;power
                  transistors;semiconductor device
                  models;semiconductor devices;semiconductor
                  diodes;semiconductor switches;spreadsheet
                  programs;IGBTs;bipolar semiconductors;conduction
                  losses;current;diodes;forward voltage drop;negative
                  temperature coefficient;paralleled semiconductor
                  chips;positive temperature coefficient;power
                  semiconductors;simulation tool;spreadsheet;switching
                  losses;temperature differences;temperature feedback},
  month =	 {Sep},
}

@INPROCEEDINGS{6808195,
  author =	 {Boelmann, C. and Weis, T.},
  booktitle =	 {Parallel and Distributed Systems (ICPADS), 2013
                  International Conference on},
  title =	 {Development of Efficient Role-Based Sensor Network
                  Applications with Excel Spreadsheets},
  year =	 2013,
  pages =	 {365-371},
  abstract =	 {Natural scientists use large scale sensor networks
                  for gathering and analyzing environmental
                  data. However, the implementation work requires
                  expert programmers. The problem is complicated by
                  limited battery lifetime, processing power and
                  memory capacity of the nodes, because this requires
                  a low-level programming language. Since scientists
                  are used to analyzing data with spreadsheets,
                  researchers have studied the possibility of applying
                  spreadsheet-based programming to sensor
                  networks. The approaches so far either require a
                  central server to execute the spreadsheet, or they
                  execute a spreadsheet run-time on each node. The
                  first approach causes higher communication cost
                  since all data has to be routed to the central
                  server and the second one causes computational
                  overhead, because evaluating a spreadsheet is slower
                  than executing handcrafted NesC-code. Hence, we
                  present a spreadsheet driven tool-chain that can
                  create efficient NesC-code and allows for simulation
                  in the spreadsheet itself. The nodes have to
                  recompute the spreadsheet formulas upon new
                  data. However, we can avoid a large fraction of this
                  recomputation by applying several optimization
                  strategies during code generation. In our example
                  scenario, sensor nodes compute the variance across a
                  series of sensor readings. We can show that the
                  optimizations save 65% CPU cycles and the code size
                  decreases by 12% when compared to non-optimized
                  execution of the spreadsheet. Thus, our approach can
                  deliver an easy way of developing sensor network
                  programs while yielding very efficient code.},
  keywords =	 {data analysis;optimisation;program
                  compilers;programming languages;spreadsheet
                  programs;wireless sensor networks;code
                  generation;environmental data analysis;environmental
                  data gathering;excel spreadsheets;handcrafted
                  NesC-code;large scale sensor networks;low-level
                  programming language;natural scientists;optimization
                  strategies;role-based sensor
                  network;Conferences;Generators;Monitoring;Optimization;Redundancy;Servers;Shift
                  registers;Computation Optimization;Designing Sensor
                  Networks;Spreadsheet Programming},
  doi =		 {10.1109/ICPADS.2013.58},
  ISSN =	 {1521-9097},
  month =	 {Dec},
}

@INPROCEEDINGS{288396,
  author =	 {Yoder, A.G. and Cohn, D.L.},
  booktitle =	 {Computer Languages, 1994., Proceedings of the 1994
                  International Conference on},
  title =	 {Real spreadsheets for real programmers},
  year =	 1994,
  pages =	 {20-30},
  abstract =	 {Spreadsheets as a computing metaphor have received
                  desultory attention from the programming languages
                  community. Efforts to map real problems onto the
                  metaphor however, have been hampered by serious
                  flaws in the design and implementation of commonly
                  available packages. We show that the intuition
                  behind such efforts is sound; we define a
                  spreadsheet mini-language, then use it to program
                  solutions to some well-known problems. Since our
                  minilanguage is in principle highly concurrent, we
                  conclude that correctly constructed spreadsheet
                  languages can offer both intuition and access to
                  parallel computational resources to their users},
  keywords =	 {parallel languages;parallel programming;spreadsheet
                  programs;concurrent language;parallel computational
                  resources;programmers;programming languages
                  community;software packages;spreadsheet
                  languages;spreadsheet
                  minilanguage;spreadsheets;Computer
                  languages;Concurrent computing;Content addressable
                  storage;Decision making;Distributed
                  computing;Inspection;Instruments;Modems;Packaging;Programming
                  profession},
  doi =		 {10.1109/ICCL.1994.288396},
  month =	 {May},
}

@INPROCEEDINGS{633125,
  author =	 {Couch, A.L. and Krumme, D.W.},
  booktitle =	 {Distributed Memory Computing Conference,
                  1991. Proceedings., The Sixth},
  title =	 {Multidimensional Spreadsheets in a Graphical
                  Symbolic Debugger for the Ncube},
  year =	 1991,
  pages =	 {205-208},
  abstract =	 {Not Available},
  keywords =	 {Computer displays;Computer
                  science;Debugging;Hardware;Multidimensional
                  systems;Operating systems;Parallel
                  programming;Prototypes;Real time
                  systems;Workstations},
  doi =		 {10.1109/DMCC.1991.633125},
  month =	 {Apr},
}

@INPROCEEDINGS{7322476,
  author =	 {Kepner, J. and Arcand, W. and Bestor, D. and
                  Bergeron, B. and Byun, C. and Edwards, L. and
                  Gadepally, V. and Hubbell, M. and Michaleas, P. and
                  Mullen, J. and Prout, A. and Rosa, A. and Yee,
                  C. and Reuther, A.},
  booktitle =	 {High Performance Extreme Computing Conference
                  (HPEC), 2015 IEEE},
  title =	 {Lustre, hadoop, accumulo},
  year =	 2015,
  pages =	 {1-5},
  abstract =	 {Data processing systems impose multiple views on
                  data as it is processed by the system. These views
                  include spreadsheets, databases, matrices, and
                  graphs. There are a wide variety of technologies
                  that can be used to store and process data through
                  these different steps. The Lustre parallel file
                  system, the Hadoop distributed file system, and the
                  Accumulo database are all designed to address the
                  largest and the most challenging data storage
                  problems. There have been many ad-hoc comparisons of
                  these technologies. This paper describes the
                  foundational principles of each technology, provides
                  simple models for assessing their capabilities, and
                  compares the various technologies on a hypothetical
                  common cluster. These comparisons indicate that
                  Lustre provides 2x more storage capacity, is less
                  likely to loose data during 3 simultaneous drive
                  failures, and provides higher bandwidth on general
                  purpose workloads. Hadoop can provide 4x greater
                  read bandwidth on special purpose
                  workloads. Accumulo provides 105 lower latency on
                  random lookups than either Lustre or Hadoop but
                  Accumulo's bulk bandwidth is 10x less. Significant
                  recent work has been done to enable mix-and-match
                  solutions that allow Lustre, Hadoop, and Accumulo to
                  be combined in different ways.},
  keywords =	 {data handling;parallel databases;parallel
                  programming;storage management;Accumulo
                  database;Hadoop distributed file system;Lustre
                  parallel file system;data processing system;data
                  storage problem;databases;drive
                  failure;graphs;matrices;random lookup;read
                  bandwidth;spreadsheets;storage
                  capacity;Bandwidth;Distributed databases;File
                  systems;Hardware;Metadata;Servers;Accumulo;Big
                  Data;Hadoop;Insider;Lustre;Parallel Performance},
  doi =		 {10.1109/HPEC.2015.7322476},
  month =	 {Sept},
}

@INPROCEEDINGS{7130265,
  author =	 {Yavuz Celikdemir, M. and Akbal, A.},
  booktitle =	 {Signal Processing and Communications Applications
                  Conference (SIU), 2015 23th},
  title =	 {Examining the structure cracks in concrete
                  structures exposed to high temperatures},
  year =	 2015,
  pages =	 {2033-2036},
  abstract =	 {Developments in image processing technology, to
                  increase the memory capacity of the computer and
                  data processing speed was accelerated in
                  parallel. Application areas of image processing is
                  increasing every day widens the field of application
                  in the construction sector. Through image
                  processing, one of the non-destructive test methods
                  for damage assessment can be made of concrete
                  structures in the economic, tenderness, and was seen
                  as a result of studies that provide ease of use in
                  terms of time.},
  keywords =	 {concrete;condition monitoring;crack detection;image
                  processing;nondestructive testing;structural
                  engineering computing;concrete
                  structures;construction sector;cracks;damage
                  assessment;data processing speed;high temperature
                  exposure;image processing technology;nondestructive
                  testing;Computers;Concrete;High definition
                  video;Image processing;MATLAB;Spreadsheet
                  programs;Determining cracks in concrete;Image
                  processing;Matlab applications},
  doi =		 {10.1109/SIU.2015.7130265},
  month =	 {May},
}

@INPROCEEDINGS{1420888,
  author =	 {Pressel, D.M. and Cronk, D. and Shende, S.S.},
  booktitle =	 {Users Group Conference (DOD_UGC'04), 2004},
  title =	 {Penvelope: a new approach to rapidly predicting the
                  performance of computationally intensive scientific
                  applications on parallel computer architectures},
  year =	 2004,
  pages =	 {298-302},
  abstract =	 {A common complaint when dealing with the performance
                  of computationally intensive scientific applications
                  on parallel computers is that programs exist to
                  predict the performance of radar systems, missiles
                  and artillery shells, drugs, etc., but no one knows
                  how to predict the performance of these applications
                  on a parallel computer. Actually, that is not quite
                  true. A more accurate statement is that no one knows
                  how to predict the performance of these applications
                  on a parallel computer in a reasonable amount of
                  time. Penvelope is an attempt to remedy this
                  situation. It is an extension to Amdahl's
                  Law/Gustafson's work on scaled speedup that takes
                  into account the cost of interprocessor
                  communication and operating system overhead, yet is
                  simple enough that it was implemented as an Excel
                  spreadsheet.},
  keywords =	 {application program interfaces;message
                  passing;parallel architectures;performance
                  evaluation;Excel spreadsheet;Penvelope;artillery
                  shells;interprocessor
                  communication;missiles;operating system
                  overhead;parallel computer architecture;radar
                  systems;scientific applications;Application
                  software;Computer applications;Computer
                  architecture;Concurrent
                  computing;Drugs;Laboratories;Military
                  computing;Missiles;Predictive models;Radar
                  applications},
  doi =		 {10.1109/DOD_UGC.2004.33},
  month =	 {June},
}

@INPROCEEDINGS{682742,
  author =	 {Rao, N.D. and Sporea, S.I. and Sawma, A.},
  booktitle =	 {Electrical and Computer Engineering, 1998. IEEE
                  Canadian Conference on},
  title =	 {Analysis of resonance problems and harmonic filter
                  design in power factor correction capacitor
                  applications},
  year =	 1998,
  volume =	 1,
  pages =	 {293-296 vol.1},
  abstract =	 {Increased use of power electronic control equipment
                  has made it necessary to pay greater attention to
                  harmonic voltages and currents in power systems. In
                  power systems containing harmonic-producing
                  equipment, application of power factor-correcting
                  capacitors may lead to a resonance condition between
                  the inductive reactance of the source and the
                  capacitive reactance of the capacitor bank. If the
                  resonance frequency occurs at or near a harmonic
                  current produced by the load, severe voltage
                  distortion and harmonic current amplification will
                  occur. Very often, the increase in harmonic current
                  is large enough to cause nuisance fuse blowing,
                  breaker tripping and overheating of equipment. The
                  paper begins with a review of the nature of harmonic
                  currents and voltages and common sources of
                  harmonic-producing equipment. This will be followed
                  by the presentation of results generated from the
                  software developed by the authors combining the
                  strengths of Mathcad and Excel spreadsheet. The
                  output from the program are the parallel resonant
                  frequencies and the resulting voltage distortion due
                  to harmonic-producing loads. In addition, the
                  program also determines the value of the series
                  inductor in the harmonic filter bank that would
                  shift the parallel resonant frequency to a value
                  less than the lowest-order harmonic term of the
                  load. Also computed are the bus voltage distortion
                  and filter harmonic duty. Work is in progress to
                  extend the scope of the software from traditional
                  passive filters to active filters},
  keywords =	 {capacitor storage;electric reactance;harmonic
                  distortion;inductors;passive filters;power
                  engineering computing;power factor correction;power
                  filters;power system harmonics;resonance;Excel
                  spreadsheet;Mathcad;active filters;breaker
                  tripping;bus voltage distortion;capacitive
                  reactance;capacitor bank;equipment
                  overheating;filter harmonic duty;harmonic current
                  amplification;harmonic currents;harmonic filter
                  bank;harmonic filter design;harmonic
                  voltages;harmonic-producing
                  equipment;harmonic-producing loads;inductive
                  reactance;lowest-order harmonic term;nuisance fuse
                  blowing;parallel resonant frequencies;passive
                  filters;power electronic control equipment;power
                  factor correction capacitor;power factor-correcting
                  capacitors;resonance frequency;resonance problems
                  analysis;series inductor;voltage distortion;Active
                  filters;Harmonic analysis;Harmonic
                  distortion;Harmonic filters;Power harmonic
                  filters;Power system analysis computing;Power system
                  harmonics;Resonance;Resonant frequency;Voltage},
  doi =		 {10.1109/CCECE.1998.682742},
  ISSN =	 {0840-7789},
  month =	 {May},
}

@INPROCEEDINGS{1467684,
  author =	 {Sarni, S. and Maciel, A. and Boulic, R. and
                  Thalmann, D.},
  booktitle =	 {Computer-Based Medical Systems,
                  2005. Proceedings. 18th IEEE Symposium on},
  title =	 {A spreadsheet framework for visual exploration of
                  biomedical datasets},
  year =	 2005,
  pages =	 {159-164},
  abstract =	 {In this paper, we present our spreadsheet framework,
                  which uses a spreadsheet-like interface for
                  exploring biomedical datasets. The principles and
                  advantages of this class of visualization systems
                  are illustrated, and a case study for the analysis
                  of hip joint congruity is presented. Throughout this
                  use case, we see how end users can compare different
                  datasets, apply parallel operations on data, create
                  analysis templates, and how this helps them in the
                  exploration process.},
  keywords =	 {data analysis;data visualisation;medical
                  computing;orthopaedics;parallel
                  processing;spreadsheet programs;biomedical
                  dataset;data analysis template;exploration
                  process;hip joint congruity;parallel
                  operation;spreadsheet framework;spreadsheet-like
                  interface;visual exploration;visualization
                  system;Application software;Biomedical imaging;Data
                  analysis;Data visualization;Flowcharts;Graphical
                  models;Hip;Libraries;Magnetic resonance
                  imaging;Virtual reality},
  doi =		 {10.1109/CBMS.2005.19},
  ISSN =	 {1063-7125},
  month =	 {June},
}

@INPROCEEDINGS{230760,
  author =	 {O'Callaghan, P.W. and Constant, B.A. and Bradford,
                  M.},
  booktitle =	 {CAD (Computer Aided Design) Tools for Thermal
                  Management, IEE Colloquium on (Digest No.027)},
  title =	 {Heat transfer in windings: CAD techniques for
                  thermal management},
  year =	 1993,
  pages =	 {6/1-6/3},
  abstract =	 {Heat transfer within and from windings has been
                  analysed using dimensionless constants and
                  spreadsheet calculations. This has enabled
                  predictions of thermal behaviour when changes in the
                  geometry or number of conductors in a group
                  alters. The effects of air voids and layers,
                  coatings and insulation, material degradation and
                  consequences of parallel and perpendicular
                  conduction can also be analysed. The results are
                  compared with those obtained using more
                  sophisticated CAD tools},
  keywords =	 {CAD;electrical engineering computing;geometry;heat
                  transfer;spreadsheet programs;thermal
                  analysis;windings;CAD techniques;air layers;air
                  voids;coatings;conductors;dimensionless
                  constants;geometry;heat transfer;insulation;material
                  degradation;parallel conduction;perpendicular
                  conduction;spreadsheet calculations;thermal
                  behaviour prediction;thermal management;windings},
  month =	 {Feb},
}

@INPROCEEDINGS{4419494,
  author =	 {Pichitlamken, J. and Uthayopas, P. and Kajkamhaeng,
                  S. and Tippayawannakorn, N.},
  booktitle =	 {Industrial Engineering and Engineering Management,
                  2007 IEEE International Conference on},
  title =	 {Service-oriented architecture on a windows cluster
                  for spreadsheet simulation},
  year =	 2007,
  pages =	 {1757-1761},
  abstract =	 {We present a proof-of-concept prototype for applying
                  service-oriented architecture (SOA) on a windows
                  cluster to spreadsheet simulation. A scalable
                  architecture based on Web services is proposed. Our
                  goal is to provide a user-friendly, yet
                  computationally powerful simulation environment for
                  end users. The experimental results show that the
                  prototype system functions in a highly scalable
                  way.},
  keywords =	 {Web services;human computer interaction;spreadsheet
                  programs;workstation clusters;SOA;Web
                  services;service-oriented architecture;spreadsheet
                  simulation;user-friendly environment;windows
                  cluster;Analytical models;Computational
                  modeling;Computer aided manufacturing;Computer
                  architecture;Computer simulation;Military
                  computing;Parallel programming;Power system
                  modeling;Service oriented architecture;Web
                  services;Windows;cluster;service-oriented
                  architecture;spreadsheet simulation},
  doi =		 {10.1109/IEEM.2007.4419494},
  month =	 {Dec},
}

@INPROCEEDINGS{4736127,
  author =	 {Pichitlamken, J. and Kajkamhaeng, S. and Uthayopas,
                  P.},
  booktitle =	 {Simulation Conference, 2008. WSC 2008. Winter},
  title =	 {High performance spreadsheet simulation on a desktop
                  grid},
  year =	 2008,
  pages =	 {663-670},
  abstract =	 {We present a proof-of-concept prototype for high
                  performance spreadsheet simulation called S3. Our
                  goal is to provide a user-friendly, yet
                  computationally powerful simulation environment for
                  end users. Our approach is to add power of parallel
                  computing on Windows-based desktop grid into popular
                  Excel models. We show that, by using standard Web
                  services and service-oriented architecture (SOA),
                  one can build a fast and efficient system on a
                  desktop grid for simulation. The complexity of
                  parallelism can be hidden from users through a
                  well-defined computation template. This work also
                  demonstrates that a massive computing power can be
                  harvested by linking off-the-shelf office PCs into a
                  desktop grid for simulation. The experimental
                  results show that the prototype system is highly
                  scalable. In the best case, the execution time can
                  be reduced 13.6 times using 16 desktop PCs; the
                  simulation time is dramatically reduced from 200
                  minutes to 14 minutes.},
  keywords =	 {Web services;digital simulation;grid computing;human
                  computer interaction;parallel processing;software
                  architecture;spreadsheet programs;Excel
                  models;S3;SOA;Web services;Windows-based desktop
                  grid;high performance spreadsheet
                  simulation;parallel computing;service-oriented
                  architecture;time 14 min;time 200 min;user-friendly
                  simulation environment;Computational
                  modeling;Concurrent computing;Grid computing;High
                  performance computing;Parallel processing;Personal
                  communication networks;Power system modeling;Service
                  oriented architecture;Virtual prototyping;Web
                  services},
  doi =		 {10.1109/WSC.2008.4736127},
  month =	 {Dec},
}

@INPROCEEDINGS{6270624,
  author =	 {Lee, V.W. and Grochowski, E. and Geva, R.},
  booktitle =	 {Parallel and Distributed Processing Symposium
                  Workshops PhD Forum (IPDPSW), 2012 IEEE 26th
                  International},
  title =	 {Performance Benefits of Heterogeneous Computing in
                  HPC Workloads},
  year =	 2012,
  pages =	 {16-26},
  abstract =	 {Chip multi-processors (CMPs) with increasing number
                  of processor cores are now becoming widely
                  available. To take advantage of many-core CMPs,
                  applications must be parallelized. However, due to
                  the nature of algorithm/programming model, some
                  parts of the application would remain
                  serial. According to Amdahl's law, the speedup of a
                  parallel application is limited by the amount of
                  serial execution it has. For a CMP with many cores,
                  this can be a serious limitation. To take full
                  advantage of the increasing number of cores, one
                  must try to reduce the execution time of the serial
                  portion of a parallel program. However, rewriting an
                  application takes time and often the return on the
                  effort invested may not justify parallelizing every
                  part of the program. Heterogeneous many-core CMP
                  design is one possible solution to support massive
                  parallel execution and to provide a reasonable
                  single-thread performance. In this paper, we use a
                  simple spreadsheet model to evaluate homogeneous and
                  heterogeneous CMP designs using execution profiles
                  of real HPC applications. Evaluated on 12 parallel
                  HPC applications, we show that heterogeneous CMPs
                  can outperform homogeneous CMPs by up to 1.35× with
                  an average speedup of 1.06× when both the
                  heterogeneous CMPs and homogeneous CMPs are
                  constrained to use the same power budget. Our study
                  found the heterogeneous CMPs can take advantage of
                  serial portion of execution that is as little as 2%
                  of total run time to provide performance
                  benefit. This suggests heterogeneous computing can
                  help mitigate the effect of not parallelizing some
                  portions of an application due to return on
                  investment concern on programming efforts.},
  keywords =	 {microprocessor chips;multiprocessing
                  systems;parallel processing;performance
                  evaluation;Amdahl law;CMP;HPC workloads;chip
                  multiprocessors;heterogeneous computing;parallel
                  application;parallel execution;parallel
                  program;performance benefits;processor cores;single
                  thread performance;spreadsheet model;Benchmark
                  testing;Computational modeling;Instruction
                  sets;Power
                  demand;Servers;Throughput;Transistors;Chip
                  multi-processors (CMPs);HPC;heterogeneous
                  computing;parallel programs},
  doi =		 {10.1109/IPDPSW.2012.18},
  month =	 {May},
}

@INPROCEEDINGS{466773,
  author =	 {Palaniappan, K. and Kambhamettu, C. and Hasler,
                  A.F. and Goldgof, D.B.},
  booktitle =	 {Computer Vision, 1995. Proceedings., Fifth
                  International Conference on},
  title =	 {Structure and semi-fluid motion analysis of
                  stereoscopic satellite images for cloud tracking},
  year =	 1995,
  pages =	 {659-665},
  abstract =	 {Time-varying multispectral observations of clouds
                  from meteorological satellites are used to estimate
                  cloud-top heights (structure) and cloud winds
                  (semi-fluid motion). Stereo image pairs over several
                  time steps were acquired by two geostationary
                  satellites with synchronized scanning
                  instruments. Cloud-top height estimation from these
                  image pairs is performed using an improved automatic
                  stereo analysis algorithm on a massively parallel
                  Maspar computer with 16 K processors. A new category
                  of motion behavior known as semi-fluid motion is
                  described for modeling cloud motions and an
                  automatic algorithm for extracting semi-fluid motion
                  is developed to track cloud winds. The time
                  sequential dense estimates of cloud-top height depth
                  maps in conjunction with intensity data are used to
                  estimate local semi-fluid motion parameters for
                  cloud tracking. Both stereo disparities and motion
                  correspondences are estimated to sub-pixel
                  accuracy. The Interactive Image SpreadSheet (IISS)
                  is a new versatile visualization tool that was
                  enhanced to analyze and visualize the results of the
                  stereo analysis and semi-fluid motion estimation
                  algorithms. Experimental results using time-varying
                  data of the visible channel from two satellites in
                  geosynchronous orbit is presented for the Hurricane
                  Frederic},
  keywords =	 {atmospheric techniques;clouds;data
                  visualisation;geophysical signal processing;motion
                  estimation;parallel machines;remote
                  sensing;spreadsheet programs;stereo image
                  processing;tracking;wind;Interactive Image
                  SpreadSheet;atmosphere meteorology time
                  steps;automatic stereo analysis algorithm;cloud
                  tracking;cloud winds;cloud-top height depth
                  maps;cloud-top height estimation;geostationary
                  satellites;intensity data;massively parallel Maspar
                  computer;meteorological satellites;motion
                  correspondences;semi-fluid motion analysis;stereo
                  disparities;stereo image pairs optical imaging
                  remote sensing;stereoscopic satellite
                  images;structure analysis;synchronized scanning
                  instruments;time sequential dense
                  estimates;time-varying multispectral
                  observations;visible channel;visualization
                  tool;Algorithm design and analysis;Clouds;Data
                  visualization;Image analysis;Image motion
                  analysis;Meteorology;Motion analysis;Motion
                  estimation;Satellites;Tracking},
  doi =		 {10.1109/ICCV.1995.466773},
  month =	 {Jun},
}

@INPROCEEDINGS{5451245,
  author =	 {Henry, E.C.A.A. and Adediran, Y.A.},
  booktitle =	 {Computer and Automation Engineering (ICCAE), 2010
                  The 2nd International Conference on},
  title =	 {Suppression of multipactor breakdown in satellite
                  rectangular waveguides using DC magnetic fields},
  year =	 2010,
  volume =	 1,
  pages =	 {766-770},
  abstract =	 {An important design consideration in a system
                  utilizing hollow waveguides operating at low gas
                  pressures and high RF power level is the prevention
                  or suppression of the multipactor breakdown
                  phenomenon. The paper investigates via a
                  mathematical model the possibility of using a DC
                  magnetic field to effect suppression of multipaction
                  in a rectangular waveguide. The result of analysis
                  and simulations of the model using MS Excel
                  Spreadsheet application suggests that a small
                  magnitude DC magnetic field perpendicular to the
                  electric field and in the wave propagation direction
                  is effective in reducing multipaction.},
  keywords =	 {electric breakdown;high-frequency
                  discharges;microwave switches;rectangular
                  waveguides;satellite communication;DC magnetic
                  fields;multipactor breakdown suppression;satellite
                  rectangular waveguides;wave propagation
                  direction;Degradation;Electric
                  breakdown;Electromagnetic waveguides;Electron
                  emission;Magnetic fields;Rectangular
                  waveguides;Rough surfaces;Satellites;Space
                  technology;Surface roughness;microwave
                  discharges;multipactor breakdown;parallel-plate
                  waveguide;secondary emission},
  doi =		 {10.1109/ICCAE.2010.5451245},
  month =	 {Feb},
}

@INPROCEEDINGS{911486,
  author =	 {Diab, H. and Abdennour, E. and Mansour, N.},
  booktitle =	 {Electronics, Circuits and Systems, 2000. ICECS
                  2000. The 7th IEEE International Conference on},
  title =	 {Spreadsheet model for MorphoSys RC array},
  year =	 2000,
  volume =	 1,
  pages =	 {66-69 vol.1},
  abstract =	 {This paper introduces reconfigurable computing and
                  MorphoSys, which is a reconfigurable system. It also
                  explains the architecture of its reconfigurable
                  hardware part. Then, it presents two spreadsheet
                  models for the operation of this reconfigurable
                  device. The first spreadsheet performs the modelling
                  through formulas, while the second does it
                  numerically. These spreadsheet models serve as
                  design and debugging tools},
  keywords =	 {cellular arrays;parallel
                  architectures;reconfigurable architectures;reduced
                  instruction set computing;spreadsheet
                  programs;MorphoSys RC array;debugging
                  tools;reconfigurable computing;spreadsheet
                  models;Application software;Application specific
                  integrated circuits;Computer
                  architecture;Debugging;Hardware;Multiplexing;Numerical
                  models;Parallel processing;Reduced instruction set
                  computing;Registers},
  doi =		 {10.1109/ICECS.2000.911486},
}

@INPROCEEDINGS{5662755,
  author =	 {Xhafa, F. and Kolodziej, J. and Bogdańsk, M.},
  booktitle =	 {P2P, Parallel, Grid, Cloud and Internet Computing
                  (3PGCIC), 2010 International Conference on},
  title =	 {A Web Interface for Meta-Heuristics Based Grid
                  Schedulers},
  year =	 2010,
  pages =	 {405-410},
  abstract =	 {The use of meta-heuristics for designing efficient
                  Grid schedulers is currently a common approach. One
                  issue related to Grid based schedulers is their
                  evaluation under different Grid configurations, such
                  as dynamics of tasks and machines, task arrival,
                  scheduling policies, etc. In this paper we present a
                  web application that interfaces the final user with
                  several meta-heuristics based Grid schedulers. The
                  application interface facilities for each user the
                  remote evaluation of the different heuristics, the
                  configuration of the schedulers as well as the
                  configuration of the Grid simulator under which the
                  schedulers are run. The simulation results and
                  traces are graphically represented and stored at the
                  server and can retrieved in different formats such
                  as spreadsheet form or pdf files. Historical
                  executions are as well kept enabling a full study of
                  use cases for different types of Grid
                  schedulers. Thus, through this application the user
                  can extract useful knowledge about the behavior of
                  different schedulers by simulating realistic
                  conditions of Grid system without needing to install
                  and configure any specific software.},
  keywords =	 {Internet;grid computing;scheduling;user
                  interfaces;Grid configurations;Web
                  interface;metaheuristics based grid scheduler;Grid
                  scheduling;Meta-heuristic;Simulation;Web interface},
  doi =		 {10.1109/3PGCIC.2010.67},
  month =	 {Nov},
}

@INPROCEEDINGS{1324012,
  author =	 {Abramson, D. and Dongarra, J. and Meek, E. and Roe,
                  P. and Shi, Z.},
  booktitle =	 {High Performance Computing and Grid in Asia Pacific
                  Region, 2004. Proceedings. Seventh International
                  Conference on},
  title =	 {Simplified grid computing through spreadsheets and
                  NetSolve},
  year =	 2004,
  pages =	 {19-24},
  abstract =	 {Grid computing has great potential but to enter the
                  mainstream it must be simplified. Tools and
                  libraries must make it easier to solve problems by
                  being simpler and at the same time more
                  sophisticated. We describe how grid computing can be
                  achieved through spreadsheets. No parallel
                  programming or complex tools need to be used. So
                  long as dependencies allow it, formulae in a
                  spreadsheet can be evaluated concurrently on the
                  grid. Thus, grid computing becomes accessible to all
                  those who can use a spreadsheet. The story is
                  completed with a sophisticated backend system,
                  NetSolve, which can solve complex linear algebra
                  systems with minimal intervention from the user. We
                  present the architecture of the system for
                  performing such simple yet sophisticated grid
                  computing and a case study which performs a large
                  singular value decomposition.},
  keywords =	 {grid computing;linear algebra;singular value
                  decomposition;spreadsheet programs;NetSolve;complex
                  linear algebra systems;grid computing;large singular
                  value decomposition;spreadsheets;system
                  architecture;Computer
                  architecture;Databases;Engines;Grid computing;High
                  performance computing;Libraries;Lifting
                  equipment;Linear algebra;Parallel
                  programming;Singular value decomposition},
  doi =		 {10.1109/HPCASIA.2004.1324012},
  month =	 {July},
}

@INPROCEEDINGS{706152,
  author =	 {Ambler, A. and Leopold, J.},
  booktitle =	 {Visual Languages, 1998. Proceedings. 1998 IEEE
                  Symposium on},
  title =	 {Public programming in a Web world},
  year =	 1998,
  pages =	 {100-107},
  abstract =	 {Web browsers have created a truly
                  platform-independent distributed environment. While
                  the main focus for this environment has been
                  pre-built applications, there is certainly an
                  opportunity for systems designed to facilitate
                  programming new applications. Systems tools, such as
                  Java, are of course readily available, but not
                  generally usable by public programmers, i.e.,
                  programmers without training in either object
                  oriented or imperative programming. What we have not
                  seen is the equivalent of a spreadsheet language,
                  designed for public programmers and making it
                  possible for these people to build applications that
                  collect and manipulate data, both from within the
                  Web and from without. We discuss the application of
                  the visual programming language, Formulate, to
                  building distributed applications via the Web
                  environment. Formulate has certain inherent
                  advantages for application to this
                  environment. Principle among these are: (1) it was
                  designed for public programmers and (2) its internal
                  evaluation algorithm is well suited for adaptation
                  to a non deterministic distributed environment using
                  symbolic URLs},
  keywords =	 {Internet;parallel programming;visual
                  languages;visual programming;Formulate;Web
                  browsers;Web environment;Web world;distributed
                  applications;internal evaluation algorithm;non
                  deterministic distributed
                  environment;platform-independent distributed
                  environment;public programmers;public
                  programming;spreadsheet language;symbolic
                  URLs;systems tools;visual programming
                  language;Algorithm design and analysis;Application
                  software;Buildings;Computer
                  languages;Internet;Java;Object oriented
                  programming;Programming profession;Uniform resource
                  locators;Visual BASIC},
  doi =		 {10.1109/VL.1998.706152},
  ISSN =	 {1049-2615},
  month =	 {Sep},
}

@INPROCEEDINGS{6272571,
  author =	 {Hassan, Q.F.},
  booktitle =	 {Computer Science and Automation Engineering (CSAE),
                  2012 IEEE International Conference on},
  title =	 {GExcel.NET: A general-purpose adapter for Excel
                  calculations on desktop grids},
  year =	 2012,
  volume =	 1,
  pages =	 {164-168},
  abstract =	 {This paper introduces a general-purpose adapter for
                  the Excel calculations known as GExcel.NET. This
                  adapter utilizes the .NET reflection and the Excel
                  automation technologies in order to enable the
                  parallel execution of the computation-intensive
                  tasks on desktop grids via Alchemi. This goal is
                  accomplished without the need to modify the
                  calculation code as GExcel.NET enables automatic
                  conversion from serial processing to parallel
                  processing at runtime. Thus, the power of GExcel.NET
                  does not only lie in enabling easy integration with
                  Excel, but also in allowing the use of existing,
                  non-grid enabled software components in a
                  grid-enabled manner.},
  keywords =	 {grid computing;mathematics computing;parallel
                  processing;spreadsheet programs;.NET
                  reflection;Alchemi;Excel automation
                  technologies;Excel
                  calculations;GExcel.NET;computation-intensive
                  tasks;desktop grids;general-purpose
                  adapter;grid-enabled manner;nongrid enabled software
                  components;parallel processing;serial
                  processing;Assembly;Computational modeling;Grid
                  computing;Portable
                  computers;Reflection;Software;.NET
                  Assemblies;Alchemi;Desktop Grids;Excel
                  Automation;Grid Computing;Reflection;Spreadsheet
                  Calculations},
  doi =		 {10.1109/CSAE.2012.6272571},
  month =	 {May},
}

@INPROCEEDINGS{6890521,
  author =	 {Ionita, D. and Bullee, J.-W. and Wieringa, R.J.},
  booktitle =	 {Evolving Security and Privacy Requirements
                  Engineering (ESPRE), 2014 IEEE 1st Workshop on},
  title =	 {Argumentation-based security requirements
                  elicitation: The next round},
  year =	 2014,
  pages =	 {7-12},
  abstract =	 {Information Security Risk Assessment can be viewed
                  as part of requirements engineering because it is
                  used to translate security goals into security
                  requirements, where security requirements are the
                  desired system properties that mitigate threats to
                  security goals. To improve the defensibility of
                  these mitigations, several researchers have
                  attempted to base risk assessment on argumentation
                  structures. However, none of these approaches have
                  so far been scalable or usable in real-world risk
                  assessments. In this paper, we present the results
                  from our search for a scalable argumentation-based
                  information security RA method. We start from
                  previous work on both formal argumentation
                  frameworks and informal argument structuring and try
                  to find a promising middle ground. An initial
                  prototype using spreadsheets is validated and
                  iteratively improved via several Case
                  Studies. Challenges such as scalability,
                  quantify-ability, ease of use, and relation to
                  existing work in parallel fields are
                  discussed. Finally, we explore the scope and
                  applicability of our approach with regard to various
                  classes of Information Systems while also drawing
                  more general conclusions on the role of
                  argumentation in security.},
  keywords =	 {risk management;security of data;argumentation
                  structures;argumentation-based information security
                  RA method;argumentation-based security requirements
                  elicitation;formal argumentation frameworks;informal
                  argument structuring;information security risk
                  assessment;requirements engineering;risk
                  assessments;security goals;spreadsheets;threat
                  mitigation;Cloud computing;Context;Games;Information
                  security;Risk management;Scalability},
  doi =		 {10.1109/ESPRE.2014.6890521},
  month =	 {Aug},
}

@INPROCEEDINGS{6883042,
  author =	 {Chang, K.S.-P. and Myers, B.A.},
  booktitle =	 {Visual Languages and Human-Centric Computing
                  (VL/HCC), 2014 IEEE Symposium on},
  title =	 {A spreadsheet model for using web service data},
  year =	 2014,
  pages =	 {169-176},
  abstract =	 {Web services offer a more reliable and efficient way
                  to access online data than scraping web
                  pages. However, web service data are often in
                  complex hierarchical structures that make it
                  difficult for people to extract the desired parts or
                  to perform any further data manipulation without
                  writing a significant amount of surprisingly
                  intricate code. In this paper, we present Gneiss, a
                  tool that extends the familiar spreadsheet metaphor
                  to support working with data returned from web
                  services. Gneiss allows users to extract the desired
                  fields in web service data using drag-and-drop, and
                  refine the results through spreadsheet formulas,
                  along with sorting and filtering the
                  data. Hierarchical data are stored as nested tables
                  in the spreadsheet and can be flattened for future
                  operations. Data flow is two-way between the
                  spreadsheet and the web services, enabling people to
                  easily make a new request by modifying spreadsheet
                  cells. In addition, using the dependency between
                  spreadsheet cells, our tool is able to create
                  parallel-running data extractions based on the
                  user's sequential demonstration. We use a set of
                  examples to demonstrate our tool's ability to create
                  fast and reusable data extraction and manipulation
                  programs that work with complex web service data.},
  keywords =	 {Web services;software tools;sorting;spreadsheet
                  programs;Gneiss;Web page scraping;Web service
                  data;data filtering;data manipulation;data
                  sorting;drag-and-drop;nested tables;online data
                  access;parallel-running data extractions;spreadsheet
                  cell modification;spreadsheet formulas;spreadsheet
                  model;two-way data flow;Data mining;Educational
                  institutions;Filtering;Motion pictures;Sorting;Web
                  pages;Web services;end-user
                  programming;mashup;spreadsheet;up},
  doi =		 {10.1109/VLHCC.2014.6883042},
  month =	 {July},
}

@INPROCEEDINGS{1179341,
  author =	 {Gezgin, C.},
  booktitle =	 {Applied Power Electronics Conference and Exposition,
                  2003. APEC '03. Eighteenth Annual IEEE},
  title =	 {A transient prediction and stability analysis tool
                  for DC-DC converters},
  year =	 2003,
  volume =	 2,
  pages =	 {1014-1020 vol.2},
  abstract =	 {An Excel-based tool that calculates both the output
                  voltage deviation V/sub max/ due to load transients
                  as well as small-signal stability characteristics of
                  a DC-DC converter is presented. Using a macromodel
                  of the converter, the tool calculates and plots the
                  voltage loop gain characteristics of a DC-DC
                  converter for a wide variety of load networks and
                  voltage-sense methods. The tool also predicts
                  stability margins of the converter+load combination
                  and estimates the under/overshoot of the regulated
                  output voltage due to a given load transient. The
                  method used is unique in the sense that 1) it
                  decouples the closed loop system into converter-only
                  and load-only subsystems which allows a very
                  convenient way of calculating loop response when the
                  load is varied; and 2) the converter-only subsystem
                  can be characterized by measured data which yields a
                  very accurate prediction of the loop response, and
                  hence the stability margin. The transient response
                  of the converter is estimated by mapping the
                  closed-loop system to a second order parallel RLC
                  network and analytically deriving the response of
                  the RLC network. The purpose of the tool is to
                  enable the converter user to get an accurate
                  estimate of stability margins and transient response
                  of the module in an application without actually
                  making measurements.},
  keywords =	 {DC-DC power convertors;RLC circuits;closed loop
                  systems;electronic engineering computing;spreadsheet
                  programs;stability;transient response;DC-DC
                  converters;Excel-based tool;RLC network;closed loop
                  system;load networks;loop response;macromodel
                  converter;small-signal stability;stability
                  analysis;stability margins;transient
                  prediction;transient response;voltage loop gain
                  characteristics;voltage-sense
                  methods;Capacitors;DC-DC power converters;Feedback
                  loop;Impedance;Power system stability;Power system
                  transients;Stability analysis;Transient
                  analysis;Transient response;Voltage},
  doi =		 {10.1109/APEC.2003.1179341},
  month =	 {Feb},
}

@INPROCEEDINGS{5350197,
  author =	 {Lunzer, A. and Fujima, J.},
  booktitle =	 {Creating, Connecting and Collaborating through
                  Computing, 2009. C5 '09. Seventh International
                  Conference on},
  title =	 {Building and Exploring with the RecipeSheet},
  year =	 2009,
  pages =	 {41-47},
  abstract =	 {The RecipeSheet is a spreadsheet-inspired
                  environment for building applications that support
                  the parallel, side-by-side exploration of
                  alternative processing results. Our early
                  investigations suggest that such an environment can
                  be valuable for educational activities in which a
                  student's understanding of some topic would be
                  improved by being able to explore and compare many
                  alternative cases. In this paper, oriented towards
                  readers who are familiar with the Squeak environment
                  on which the RecipeSheet has been built, we explain
                  and illustrate the facilities available to end users
                  for building their own applications. This is a step
                  towards a release of the RecipeSheet to the
                  community of educators who are excited by the
                  general promise of Squeak, and the various kinds of
                  tool that can be built with it.},
  keywords =	 {computer aided instruction;software
                  engineering;spreadsheet programs;RecipeSheet;Squeak
                  environment;educational
                  activities;spreadsheet-inspired
                  environment;International collaboration;Joining
                  processes},
  doi =		 {10.1109/C5.2009.21},
  month =	 {Jan},
}

@INPROCEEDINGS{5731071,
  author =	 {Breitigam, D.},
  booktitle =	 {Digital Avionics Systems Conference, 2003. DASC
                  '03. The 22nd},
  title =	 {A reusable, event-based architecture for business
                  rule models},
  year =	 2003,
  volume =	 1,
  pages =	 {3.A.2-3.1-8 vol.1},
  abstract =	 {The paper presents a reusable, event-based
                  architecture for business rule models. This paper
                  uses the term Cellular Event-Based Architecture
                  (CEBA) to refer to the structure and attributes
                  internal to the software. The term granularity is
                  used widely in the software architecture literature
                  to refer to the size of objects in a software
                  design. This paper substitutes the word cellular for
                  the phrase fine granularity, both because it hints
                  at the parallels with the spreadsheet cell, and also
                  because it fits very well with the dictionary
                  definition of cellular: relating to small parts or
                  groups making up a whole."" This paper promotes the
                  use of the cell as a design pattern in the
                  implementation of the business rules layer in a
                  high-order language."},
  keywords =	 {object-oriented programming;software development
                  management;weapons;CEBA;business rule model;cellular
                  event-based architecture;high-order
                  language;object-oriented methods;software design},
  doi =		 {10.1109/DASC.2003.1245821},
  month =	 {Oct},
}

@INPROCEEDINGS{116017,
  author =	 {Sutherland, A.M. and Campbell, M. and Ariki, Y. and
                  Jack, M.},
  booktitle =	 {Acoustics, Speech, and Signal Processing,
                  1990. ICASSP-90., 1990 International Conference on},
  title =	 {OSPREY: a transputer based continuous speech
                  recognition system},
  year =	 1990,
  pages =	 {949-952 vol.2},
  abstract =	 {A real-time continuous speech recognition system
                  based on the Inmos transputer is developed. The
                  recognition algorithm is described in detail, and
                  the hardware architecture of the system (which
                  incorporates the AT&T DSP32-C and Inmos T800
                  transputers) is also described. The implementation
                  of the hidden Markov model recognition algorithm in
                  parallel form is presented. The system has been
                  built using readily available hardware, and it
                  operates using an IBM-PC host, providing real-time
                  speaker-dependent recognition for aircraft movement
                  and spreadsheet control applications},
  keywords =	 {Markov processes;computerised signal
                  processing;parallel algorithms;parallel
                  architectures;real-time systems;speech analysis and
                  processing;speech
                  recognition;transputers;DSP32-C;IBM-PC host;Inmos
                  T800;Inmos transputer;OSPREY;aircraft
                  movement;continuous speech recognition
                  system;hardware architecture;hidden Markov
                  model;parallel form;real-time;recognition
                  algorithm;speaker-dependent recognition;spreadsheet
                  control applications;Aerospace
                  control;Aircraft;Control
                  systems;Decoding;Hardware;Hidden Markov
                  models;Lattices;Lifting equipment;Real time
                  systems;Speech recognition},
  doi =		 {10.1109/ICASSP.1990.116017},
  ISSN =	 {1520-6149},
  month =	 {Apr},
}

@INPROCEEDINGS{869083,
  author =	 {Srisathapornphat, C. and Jaikaeo, C. and Chien-Chung
                  Shen},
  booktitle =	 {Parallel Processing, 2000. Proceedings. 2000
                  International Workshops on},
  title =	 {Sensor Information Networking Architecture},
  year =	 2000,
  pages =	 {23-30},
  abstract =	 {The advent of technology has facilitated the
                  development of networked systems of extremely small,
                  low-power devices that combine programmable
                  general-purpose computing with multiple sensing and
                  wireless communication capability. This networked
                  system of programmable sensor nodes, which together
                  form a sensor network, poses unique challenges on
                  how information collected by, and stored within, the
                  sensor network could be queried and accessed, and
                  how concurrent sensing tasks could be executed
                  internally and programmed by external users. In this
                  paper, we describe SINA (Sensor Information
                  Networking Architecture), which facilitates the
                  querying, monitoring and tasking of sensor
                  networks. We model a sensor network as a collection
                  of massively distributed objects, and SINA plays the
                  role of middleware that facilitates the adaptive
                  organization of sensor information. The SINA kernel
                  provides a set of configuration and communication
                  primitives that enable the scalable, robust and
                  energy-efficient organization of, and interactions
                  among, sensor objects. On top of the SINA kernel is
                  a programmable substrate that follows the
                  spreadsheet paradigm and provides mechanisms to
                  create associations among sensor nodes. Users then
                  access information within a sensor network using
                  declarative queries and perform tasks using
                  programmable scripts. Issues concerning interworking
                  between stationary sensor networks and mobile nodes
                  are also addressed},
  keywords =	 {client-server systems;query processing;sensor
                  fusion;wireless LAN;SINA kernel;Sensor Information
                  Networking Architecture;adaptive information
                  organization;communication primitives;concurrent
                  sensing tasks;configuration primitives;declarative
                  queries;information access;internal task
                  execution;interworking;low-power devices;massively
                  distributed objects;middleware;mobile nodes;network
                  monitoring;network querying;networked
                  systems;programmable general-purpose
                  computing;programmable scripts;programmable sensor
                  nodes;programmable substrate;scalable robust
                  energy-efficient organization;sensor network;sensor
                  node associations;sensor object
                  interactions;spreadsheet paradigm;stationary sensor
                  networks;tasking;wireless communication;Computer
                  architecture;Computer networks;Energy
                  efficiency;Kernel;Middleware;Monitoring;Robustness;Sensor
                  phenomena and characterization;Sensor
                  systems;Wireless communication},
  doi =		 {10.1109/ICPPW.2000.869083},
  ISSN =	 {1530-2016},
}
